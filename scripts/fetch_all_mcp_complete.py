#!/usr/bin/env python3
import subprocess
import json
import time

COOKIE = 'cna=cAShIM33lxUCAXPOep8X1ti3; _ga=GA1.1.1010479972.1746540150; h_uid=2219791990034; csrf_token=4wNSbp0Rx2qL7QIeWMYUR6r6cOU%3D; t=1a5b1f2f6a81129bca720a75c6e98490; acw_tc=0bcd4c5217607177651504648e7dc32b48956f923e1f88ec7097cfa66398f6; isg=BC8v8h3Hm1RmnJ-AJ5JV8kybvkU51IP2JNSSq0G8yR6lkE-SSaQoR-sDE4Cuwltu; acw_sc__v2=197d84838-bdc8c4c72ef03ea26cbc2a4929e43eed57d6ed582674296576'
CSRF_TOKEN = '4wNSbp0Rx2qL7QIeWMYUR6r6cOU='

def fetch_page(page_num, page_size=100):
    curl_cmd = [
        'curl', '-s',
        'https://modelscope.cn/api/v1/dolphin/mcpServers',
        '-X', 'PUT',
        '-H', 'Accept: application/json, text/plain, */*',
        '-H', 'Content-Type: application/json',
        '-b', COOKIE,
        '-H', f'X-CSRF-TOKEN: {CSRF_TOKEN}',
        '-H', 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        '--data-raw', json.dumps({
            "PageSize": page_size,
            "PageNumber": page_num,
            "Query": "",
            "Criterion": []
        })
    ]

    result = subprocess.run(curl_cmd, capture_output=True, text=True, timeout=30)
    return json.loads(result.stdout)

print("å¼€å§‹è·å–å…¨éƒ¨MCPæœåŠ¡æ•°æ®...")

all_servers = []
categories = []
total_count = 0

# ç¬¬ä¸€é¡µè·å–åˆ†ç±»ä¿¡æ¯
print("\næ­£åœ¨è·å–åˆ†ç±»ä¿¡æ¯...")
try:
    response = fetch_page(1, 100)
    if response.get('Success'):
        data = response.get('Data', {})
        categories = data.get('FiledAgg', {}).get('Category', [])
        print(f"âœ“ è·å–åˆ° {len(categories)} ä¸ªåˆ†ç±»")

        mcp_data = data.get('McpServer', {})
        total_count = mcp_data.get('TotalCount', 0)
        print(f"âœ“ æ€»æœåŠ¡æ•°: {total_count}")
except Exception as e:
    print(f"âœ— é”™è¯¯: {e}")
    exit(1)

# è®¡ç®—éœ€è¦è·å–çš„é¡µæ•°
total_pages = (total_count + 99) // 100
print(f"\néœ€è¦è·å– {total_pages} é¡µæ•°æ®")

# è·å–æ‰€æœ‰é¡µé¢
max_pages = min(total_pages, 60)  # æœ€å¤šè·å–60é¡µï¼Œé¿å…è¯·æ±‚è¿‡å¤š
print(f"å°†è·å–å‰ {max_pages} é¡µï¼ˆå…± {max_pages * 100} æ¡æ•°æ®ï¼‰")

for page in range(1, max_pages + 1):
    print(f"\ræ­£åœ¨è·å–ç¬¬ {page}/{max_pages} é¡µ...", end='', flush=True)

    try:
        response = fetch_page(page, 100)

        if response.get('Success'):
            mcp_data = response.get('Data', {}).get('McpServer', {})
            servers = mcp_data.get('McpServers', [])

            if servers:
                all_servers.extend(servers)
            else:
                print(f"\nç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢è·å–")
                break
        else:
            print(f"\nç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼Œåœæ­¢è·å–")
            break

    except Exception as e:
        print(f"\nç¬¬ {page} é¡µé”™è¯¯: {e}ï¼Œåœæ­¢è·å–")
        break

    # æ¯10é¡µå»¶è¿Ÿç¨é•¿ä¸€äº›
    if page % 10 == 0:
        time.sleep(1)
    else:
        time.sleep(0.3)

print(f"\n\nè·å–å®Œæˆï¼")

# ä¿å­˜æ•°æ®
output = {
    "categories": categories,
    "servers": all_servers,
    "totalCount": total_count,
    "fetchedCount": len(all_servers)
}

with open('src/data/mcp-data.json', 'w', encoding='utf-8') as f:
    json.dump(output, f, ensure_ascii=False, indent=2)

print("\n" + "="*60)
print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° src/data/mcp-data.json")
print(f"ğŸ“Š è·å– {len(all_servers)}/{total_count} æ¡æœåŠ¡")
print(f"ğŸ“‚ {len(categories)} ä¸ªåˆ†ç±»")
print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {len(json.dumps(output, ensure_ascii=False)) / 1024 / 1024:.2f} MB")
print("="*60)

